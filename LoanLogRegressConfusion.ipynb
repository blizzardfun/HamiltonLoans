{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "respondent IDs for each lender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# banks\n",
    "USBank= 504713  # US Bank\n",
    "Wells=451965  #Wells Fargo\n",
    "Bell=19581    # Bell Bank\n",
    "\n",
    "# online lenders\n",
    "Quicken=7197000003        #Quicken Loan\n",
    "AMEC=411941324             #American Mortgage & Equity Consultants Inc.\n",
    "Guaranteed=364327855      # Guaranteed Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConfusionSpread (cnf_matrix):\n",
    "#############################################\n",
    "# returns the confusion matrix converted to percent of a whole \n",
    "# returned as a list positions [00,01, 10,11]\n",
    "########################################33\n",
    "    print(\"confusion matrix\")\n",
    "    print(cnf_matrix)\n",
    "    \n",
    "    sum = cnf_matrix[0][0]+cnf_matrix[0][1]+cnf_matrix[1][0]+cnf_matrix[1][1]\n",
    "\n",
    "    cnf_list=[cnf_matrix[0][0]/sum,cnf_matrix[0][1]/sum,cnf_matrix[1][0]/sum,cnf_matrix[1][1]/sum]\n",
    "    return cnf_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def successrate(y_values):\n",
    "    success=sum(y_values)\n",
    "    total = len(y_values)\n",
    "    return success/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogRegress(X_train, y_train,X_test,y_test):\n",
    "#####################################\n",
    "# use Logistic Regression to train and test a model\n",
    "# returning test score\n",
    "\n",
    "    # create model\n",
    "    LRmodel = LogisticRegression()\n",
    "    # fit model with training data\n",
    "#     print(\"X-train\",X_train.head())\n",
    "#     print(\"y_train\",y_train.head())\n",
    "    LRmodel.fit(X_train, y_train)\n",
    "\n",
    "       # confusion matrix \n",
    "    y_pred=LRmodel.predict(X_test)\n",
    "    cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    cnf_result=ConfusionSpread(cnf_matrix) # convert confusion matrix to list of percents\n",
    "    print(\"training loan success\",successrate(y_train))\n",
    "    print(\"testing loan success\",successrate(y_test))  \n",
    "    print(\"predicted loan success\",successrate(y_pred)) \n",
    "    \n",
    "    # validate the model using testing data\n",
    "    test_score=LRmodel.score(X_test, y_test)\n",
    "#    params=LRmodel.get_params(deep=False)\n",
    "#    print(\"test_score\",test_score)\n",
    "#    print(\"params=\",params)\n",
    "    return cnf_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AssignData(set, data_df):\n",
    "##############################################\n",
    "# choose the lender group for test\n",
    "# set = 0 use all of data_df this is all lenders\n",
    "# set = 1 select from data_df for 'respondent id' in usbank, wells, quicken\n",
    "# set > 2 select from data_df for 'respondent id' = set\n",
    "# split for train and test data\n",
    "# call LogRegress\n",
    "\n",
    "    #reduce the data used for set > 0 to a subset of lenders or a particular lender\n",
    "    if set > 2 :\n",
    "        data_df=data_df.loc[data_df['respondent id'] == set,:]\n",
    "    elif set == 1:\n",
    "        data_df=data_df.loc[(data_df['respondent id'] == USBank) | (data_df['respondent id'] == Wells) | (data_df['respondent id'] == Quicken), :]\n",
    "           \n",
    "        # Assign X (data) and y (target)\n",
    "    X=data_df.drop([\"action modified\",\"respondent id\"],axis=1)\n",
    "    y=data_df[\"action modified\"]\n",
    "#    print(\"shape\", X.shape, y.shape)  , stratify=y\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "\n",
    "    return LogRegress(X_train, y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AssignData2(set, data_df,test_df):\n",
    "##############################################\n",
    "# FOR USE WHEN TRAINING AND TESTING ON DIFFERENT SETS\n",
    "# choose the lender group for BOTH train and test data\n",
    "# set = 0 use all of data_df this is all lenders\n",
    "# set = 1 select from data_df for 'respondent id' in usbank, wells quicken\n",
    "# set > 2 select from data_df for 'respondent id' = set\n",
    "# call LogRegress\n",
    "\n",
    "\n",
    "    #reduce the data used for set > 0 to a subset of lenders or a particular lender\n",
    "    if set > 2 :\n",
    "        data_df=data_df.loc[data_df['respondent id'] == set,:]\n",
    "        test_df=test_df.loc[test_df['respondent id'] == set,:]\n",
    "    elif set == 1:\n",
    "        data_df=data_df.loc[(data_df['respondent id'] == USBank) | (data_df['respondent id'] == Wells) | (data_df['respondent id'] == Quicken), :]\n",
    "        test_df=test_df.loc[(test_df['respondent id'] == USBank) | (test_df['respondent id'] == Wells) | (test_df['respondent id'] == Quicken), :]\n",
    "           \n",
    "        # Assign X (data) and y (target) for training\n",
    "    X_train=data_df.drop([\"action modified\",\"respondent id\"],axis=1)\n",
    "    y_train=data_df[\"action modified\"]\n",
    "    \n",
    "            # Assign X (data) and y (target) for test\n",
    "    X_test=test_df.drop([\"action modified\",\"respondent id\"],axis=1)\n",
    "    y_test=test_df[\"action modified\"]\n",
    "#    print(\"shape\", X.shape, y.shape)\n",
    "\n",
    "\n",
    "    return LogRegress(X_train, y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LenderTest(test_data,testname):\n",
    "#################################################\n",
    "# goes through the 4 tests of lender groups with one version of the test data\n",
    "# all 3 usbank, wells, quicken\n",
    "#  USBANK \n",
    "# Wells \n",
    "# Quicken\n",
    "# returns the results in list\n",
    "############################################################\n",
    "\n",
    "    # for returning confusion matrix\n",
    "    result_dict={}\n",
    "    # get result for all 3\n",
    "    print(\"all\")\n",
    "    result_dict['all'+ testname]=AssignData(1,test_data)\n",
    "    # result for USBank \n",
    "    print(\"usbank\")\n",
    "    result_dict['usbank'+ testname]=AssignData(USBank,test_data)\n",
    "        # result for Wells\n",
    "    print(\"wells\")\n",
    "    result_dict['wells'+ testname]=AssignData(Wells,test_data)\n",
    "    #result for Quicken\n",
    "    print(\"quicken\")\n",
    "    result_dict['quicken'+ testname]=AssignData(Quicken,test_data)\n",
    "\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LenderTest2(train_data,test_data,testname):\n",
    "#################################################\n",
    "# goes through the 4 tests of lender groups with one version of the test data\n",
    "# all 3 usbank, wells, quicken\n",
    "#  USBANK \n",
    "# Wells \n",
    "# Quicken\n",
    "# returns the results in list\n",
    "############################################################\n",
    "\n",
    "# for returning confusion matirx\n",
    "    result_dict={}\n",
    "    # get result for all 3\n",
    "    print(\"all\")\n",
    "    result_dict['all'+ testname]=AssignData2(1,train_data,test_data)\n",
    "    # result for USBank, \n",
    "    print(\"usbank\")\n",
    "    result_dict['usbank'+ testname]=AssignData2(USBank,train_data,test_data)\n",
    "        # result for Wells\n",
    "    print(\"wells\")\n",
    "    result_dict['wells'+ testname]=AssignData2(Wells,train_data,test_data)\n",
    "    #result for Quicken\n",
    "    print(\"quicken\")\n",
    "    result_dict['quicken'+ testname]=AssignData2(Quicken,train_data,test_data)\n",
    "\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main body of code \n",
    "read in data file\n",
    "\n",
    "prepare training data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "file=\"AllData2017.csv\"\n",
    "loan_data=pd.read_csv(file)\n",
    "\n",
    "loan_data=loan_data[[\"respondent id\",\"agency code\",\"property type\",\"loan purpose\",\"loan amount\",\\\n",
    "                     \"applicant race 1\",\"sex\",\"lien status\",\"loan type modified\",\\\n",
    "                     \"action modified\",\"income cleaned\",\"income loan ratio\"]]\n",
    "\n",
    "race_sex_less_data=loan_data.drop([\"sex\",\"applicant race 1\"], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prepare unique group test data sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single gender  - genderless data\n",
    "female_data=loan_data[loan_data[\"sex\"] == 2]\n",
    "female_data=female_data.drop([\"sex\",\"applicant race 1\"],axis=1)\n",
    "male_data=loan_data[loan_data[\"sex\"] == 1]\n",
    "male_data=male_data.drop([\"sex\",\"applicant race 1\"],axis=1)\n",
    "\n",
    "#single race - raceless data\n",
    "asian_data=loan_data[loan_data[\"applicant race 1\"] == 2]\n",
    "asian_data=asian_data.drop([\"sex\",\"applicant race 1\"],axis=1)\n",
    "\n",
    "black_data=loan_data[loan_data[\"applicant race 1\"] == 3]\n",
    "black_data=black_data.drop([\"sex\",\"applicant race 1\"],axis=1)\n",
    "\n",
    "white_data=loan_data[loan_data[\"applicant race 1\"] == 5]\n",
    "white_data=white_data.drop([\"sex\",\"applicant race 1\"],axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run tests and concat lender sets to dataframe\n",
    "run for 3 lenders, as a group and individually\n",
    "\n",
    "all data - train and test with same data\n",
    "\n",
    "raceless and sexless (blind) data - train and test same data\n",
    "\n",
    "asian -train with blind training data , test with unique group test data\n",
    "\n",
    "black -train with blind training data , test with unique group test data \n",
    "\n",
    "white -train with blind training data , test with unique group test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "all\n",
      "confusion matrix\n",
      "[[ 112 1839]\n",
      " [  54 6513]]\n",
      "training loan success 0.7748992290533401\n",
      "testing loan success 0.7709556233857713\n",
      "predicted loan success 0.9805118572434844\n",
      "usbank\n",
      "confusion matrix\n",
      "[[  95  540]\n",
      " [  91 1948]]\n",
      "training loan success 0.7518703241895262\n",
      "testing loan success 0.762528047868362\n",
      "predicted loan success 0.9304412864622289\n",
      "wells\n",
      "confusion matrix\n",
      "[[  69  632]\n",
      " [  22 3026]]\n",
      "training loan success 0.8114163777007202\n",
      "testing loan success 0.813016804481195\n",
      "predicted loan success 0.9757268604961323\n",
      "quicken\n",
      "confusion matrix\n",
      "[[   0  565]\n",
      " [   0 1531]]\n",
      "training loan success 0.7309466984884646\n",
      "testing loan success 0.7304389312977099\n",
      "predicted loan success 1.0\n",
      "blind\n",
      "all\n",
      "confusion matrix\n",
      "[[  71 1880]\n",
      " [  28 6539]]\n",
      "training loan success 0.7748992290533401\n",
      "testing loan success 0.7709556233857713\n",
      "predicted loan success 0.9883775534162949\n",
      "usbank\n",
      "confusion matrix\n",
      "[[  99  536]\n",
      " [ 110 1929]]\n",
      "training loan success 0.7518703241895262\n",
      "testing loan success 0.762528047868362\n",
      "predicted loan success 0.9218399401645475\n",
      "wells\n",
      "confusion matrix\n",
      "[[  55  646]\n",
      " [  16 3032]]\n",
      "training loan success 0.8114163777007202\n",
      "testing loan success 0.813016804481195\n",
      "predicted loan success 0.9810616164310483\n",
      "quicken\n",
      "confusion matrix\n",
      "[[   0  565]\n",
      " [   0 1531]]\n",
      "training loan success 0.7309466984884646\n",
      "testing loan success 0.7304389312977099\n",
      "predicted loan success 1.0\n",
      "female\n",
      "all\n",
      "confusion matrix\n",
      "[[ 110 2254]\n",
      " [  41 7537]]\n",
      "training loan success 0.773913298699774\n",
      "testing loan success 0.7622208811104405\n",
      "predicted loan success 0.9848119090726212\n",
      "usbank\n",
      "confusion matrix\n",
      "[[ 184  720]\n",
      " [ 135 2307]]\n",
      "training loan success 0.7545352534131289\n",
      "testing loan success 0.7298266586969516\n",
      "predicted loan success 0.9046622833233712\n",
      "wells\n",
      "confusion matrix\n",
      "[[  95  880]\n",
      " [  29 3620]]\n",
      "training loan success 0.8118164843958389\n",
      "testing loan success 0.789143598615917\n",
      "predicted loan success 0.9731833910034602\n",
      "quicken\n",
      "confusion matrix\n",
      "[[   0  485]\n",
      " [   0 1487]]\n",
      "training loan success 0.7308197112516406\n",
      "testing loan success 0.7540567951318459\n",
      "predicted loan success 1.0\n",
      "male\n",
      "all\n",
      "confusion matrix\n",
      "[[  151  4001]\n",
      " [   69 15718]]\n",
      "training loan success 0.773913298699774\n",
      "testing loan success 0.7917648828928231\n",
      "predicted loan success 0.9889663473594463\n",
      "usbank\n",
      "confusion matrix\n",
      "[[ 195 1242]\n",
      " [ 178 4868]]\n",
      "training loan success 0.7545352534131289\n",
      "testing loan success 0.7783433595557612\n",
      "predicted loan success 0.9424649082215024\n",
      "wells\n",
      "confusion matrix\n",
      "[[ 138 1539]\n",
      " [  39 7848]]\n",
      "training loan success 0.8118164843958389\n",
      "testing loan success 0.8246549560853199\n",
      "predicted loan success 0.9814930991217063\n",
      "quicken\n",
      "confusion matrix\n",
      "[[   0 1038]\n",
      " [   0 2854]]\n",
      "training loan success 0.7308197112516406\n",
      "testing loan success 0.7332990750256937\n",
      "predicted loan success 1.0\n",
      "asian\n",
      "all\n",
      "confusion matrix\n",
      "[[ 18 288]\n",
      " [  2 927]]\n",
      "training loan success 0.773913298699774\n",
      "testing loan success 0.7522267206477733\n",
      "predicted loan success 0.9838056680161943\n",
      "usbank\n",
      "confusion matrix\n",
      "[[ 21 107]\n",
      " [  8 329]]\n",
      "training loan success 0.7545352534131289\n",
      "testing loan success 0.7247311827956989\n",
      "predicted loan success 0.9376344086021505\n",
      "wells\n",
      "confusion matrix\n",
      "[[ 19 124]\n",
      " [  3 493]]\n",
      "training loan success 0.8118164843958389\n",
      "testing loan success 0.7762128325508607\n",
      "predicted loan success 0.9655712050078247\n",
      "quicken\n",
      "confusion matrix\n",
      "[[ 0 35]\n",
      " [ 0 96]]\n",
      "training loan success 0.7308197112516406\n",
      "testing loan success 0.732824427480916\n",
      "predicted loan success 1.0\n",
      "black\n",
      "all\n",
      "confusion matrix\n",
      "[[ 17 310]\n",
      " [  4 571]]\n",
      "training loan success 0.773913298699774\n",
      "testing loan success 0.6374722838137472\n",
      "predicted loan success 0.9767184035476718\n",
      "usbank\n",
      "confusion matrix\n",
      "[[ 32  99]\n",
      " [ 16 151]]\n",
      "training loan success 0.7545352534131289\n",
      "testing loan success 0.5604026845637584\n",
      "predicted loan success 0.8389261744966443\n",
      "wells\n",
      "confusion matrix\n",
      "[[ 15 138]\n",
      " [  4 307]]\n",
      "training loan success 0.8118164843958389\n",
      "testing loan success 0.6702586206896551\n",
      "predicted loan success 0.959051724137931\n",
      "quicken\n",
      "confusion matrix\n",
      "[[ 0 43]\n",
      " [ 0 97]]\n",
      "training loan success 0.7308197112516406\n",
      "testing loan success 0.6928571428571428\n",
      "predicted loan success 1.0\n",
      "white\n",
      "all\n",
      "confusion matrix\n",
      "[[  212  5291]\n",
      " [  101 20861]]\n",
      "training loan success 0.773913298699774\n",
      "testing loan success 0.7920649914982052\n",
      "predicted loan success 0.9881730587568487\n",
      "usbank\n",
      "confusion matrix\n",
      "[[ 311 1678]\n",
      " [ 284 6611]]\n",
      "training loan success 0.7545352534131289\n",
      "testing loan success 0.7761143628995948\n",
      "predicted loan success 0.9330256641152634\n",
      "wells\n",
      "confusion matrix\n",
      "[[  186  2087]\n",
      " [   59 10511]]\n",
      "training loan success 0.8118164843958389\n",
      "testing loan success 0.8230164291832126\n",
      "predicted loan success 0.9809234602507202\n",
      "quicken\n",
      "confusion matrix\n",
      "[[   0 1241]\n",
      " [   0 3497]]\n",
      "training loan success 0.7308197112516406\n",
      "testing loan success 0.7380751371886872\n",
      "predicted loan success 1.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>all_test</th>\n",
       "      <th>usbank_test</th>\n",
       "      <th>wells_test</th>\n",
       "      <th>quicken_test</th>\n",
       "      <th>all_blindtest</th>\n",
       "      <th>usbank_blindtest</th>\n",
       "      <th>wells_blindtest</th>\n",
       "      <th>quicken_blindtest</th>\n",
       "      <th>all_female</th>\n",
       "      <th>usbank_female</th>\n",
       "      <th>...</th>\n",
       "      <th>wells_asian</th>\n",
       "      <th>quicken_asian</th>\n",
       "      <th>all_black</th>\n",
       "      <th>usbank_black</th>\n",
       "      <th>wells_black</th>\n",
       "      <th>quicken_black</th>\n",
       "      <th>all_white</th>\n",
       "      <th>usbank_white</th>\n",
       "      <th>wells_white</th>\n",
       "      <th>quicken_white</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>true-</th>\n",
       "      <td>0.013149</td>\n",
       "      <td>0.035527</td>\n",
       "      <td>0.018405</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008335</td>\n",
       "      <td>0.037023</td>\n",
       "      <td>0.014671</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011064</td>\n",
       "      <td>0.054991</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029734</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018847</td>\n",
       "      <td>0.107383</td>\n",
       "      <td>0.032328</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008011</td>\n",
       "      <td>0.035007</td>\n",
       "      <td>0.014483</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>false-</th>\n",
       "      <td>0.215896</td>\n",
       "      <td>0.201945</td>\n",
       "      <td>0.168578</td>\n",
       "      <td>0.269561</td>\n",
       "      <td>0.220709</td>\n",
       "      <td>0.200449</td>\n",
       "      <td>0.172313</td>\n",
       "      <td>0.269561</td>\n",
       "      <td>0.226715</td>\n",
       "      <td>0.215182</td>\n",
       "      <td>...</td>\n",
       "      <td>0.194053</td>\n",
       "      <td>0.267176</td>\n",
       "      <td>0.343681</td>\n",
       "      <td>0.332215</td>\n",
       "      <td>0.297414</td>\n",
       "      <td>0.307143</td>\n",
       "      <td>0.199924</td>\n",
       "      <td>0.188879</td>\n",
       "      <td>0.162501</td>\n",
       "      <td>0.261925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>false+</th>\n",
       "      <td>0.006340</td>\n",
       "      <td>0.034031</td>\n",
       "      <td>0.005868</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003287</td>\n",
       "      <td>0.041137</td>\n",
       "      <td>0.004268</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004124</td>\n",
       "      <td>0.040347</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004695</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004435</td>\n",
       "      <td>0.053691</td>\n",
       "      <td>0.008621</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003816</td>\n",
       "      <td>0.031968</td>\n",
       "      <td>0.004594</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>true+</th>\n",
       "      <td>0.764616</td>\n",
       "      <td>0.728497</td>\n",
       "      <td>0.807149</td>\n",
       "      <td>0.730439</td>\n",
       "      <td>0.767668</td>\n",
       "      <td>0.721391</td>\n",
       "      <td>0.808749</td>\n",
       "      <td>0.730439</td>\n",
       "      <td>0.758097</td>\n",
       "      <td>0.689480</td>\n",
       "      <td>...</td>\n",
       "      <td>0.771518</td>\n",
       "      <td>0.732824</td>\n",
       "      <td>0.633038</td>\n",
       "      <td>0.506711</td>\n",
       "      <td>0.661638</td>\n",
       "      <td>0.692857</td>\n",
       "      <td>0.788249</td>\n",
       "      <td>0.744147</td>\n",
       "      <td>0.818422</td>\n",
       "      <td>0.738075</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        all_test  usbank_test  wells_test  quicken_test  all_blindtest  \\\n",
       "true-   0.013149     0.035527    0.018405      0.000000       0.008335   \n",
       "false-  0.215896     0.201945    0.168578      0.269561       0.220709   \n",
       "false+  0.006340     0.034031    0.005868      0.000000       0.003287   \n",
       "true+   0.764616     0.728497    0.807149      0.730439       0.767668   \n",
       "\n",
       "        usbank_blindtest  wells_blindtest  quicken_blindtest  all_female  \\\n",
       "true-           0.037023         0.014671           0.000000    0.011064   \n",
       "false-          0.200449         0.172313           0.269561    0.226715   \n",
       "false+          0.041137         0.004268           0.000000    0.004124   \n",
       "true+           0.721391         0.808749           0.730439    0.758097   \n",
       "\n",
       "        usbank_female      ...        wells_asian  quicken_asian  all_black  \\\n",
       "true-        0.054991      ...           0.029734       0.000000   0.018847   \n",
       "false-       0.215182      ...           0.194053       0.267176   0.343681   \n",
       "false+       0.040347      ...           0.004695       0.000000   0.004435   \n",
       "true+        0.689480      ...           0.771518       0.732824   0.633038   \n",
       "\n",
       "        usbank_black  wells_black  quicken_black  all_white  usbank_white  \\\n",
       "true-       0.107383     0.032328       0.000000   0.008011      0.035007   \n",
       "false-      0.332215     0.297414       0.307143   0.199924      0.188879   \n",
       "false+      0.053691     0.008621       0.000000   0.003816      0.031968   \n",
       "true+       0.506711     0.661638       0.692857   0.788249      0.744147   \n",
       "\n",
       "        wells_white  quicken_white  \n",
       "true-      0.014483       0.000000  \n",
       "false-     0.162501       0.261925  \n",
       "false+     0.004594       0.000000  \n",
       "true+      0.818422       0.738075  \n",
       "\n",
       "[4 rows x 28 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    #race gender included\n",
    "print(\"test\")\n",
    "result_df=pd.DataFrame(LenderTest(loan_data,'_test'), index=['true-','false-','false+','true+'])\n",
    "    #blind test norace or gender\n",
    "print(\"blind\")\n",
    "confusion_df=pd.DataFrame(LenderTest(race_sex_less_data,'_blindtest'), index=['true-','false-','false+','true+'])\n",
    "result_df=pd.concat([result_df, confusion_df],axis=1)\n",
    "    # results for the effectof gender -female\n",
    "print(\"female\")\n",
    "confusion_df=pd.DataFrame(LenderTest2(race_sex_less_data,female_data,'_female'), index=['true-','false-','false+','true+'])\n",
    "result_df=pd.concat([result_df, confusion_df],axis=1)\n",
    "    # results for the effectof gender -Male\n",
    "print(\"male\")\n",
    "confusion_df=pd.DataFrame(LenderTest2(race_sex_less_data,male_data,'_male'), index=['true-','false-','false+','true+'])\n",
    "result_df=pd.concat([result_df, confusion_df],axis=1)\n",
    "\n",
    "#results for the effect of race asian\n",
    "print(\"asian\")\n",
    "confusion_df=pd.DataFrame(LenderTest2(race_sex_less_data,asian_data,'_asian'), index=['true-','false-','false+','true+'])\n",
    "result_df=pd.concat([result_df, confusion_df],axis=1)\n",
    "# #results for the effect of race black\n",
    "print(\"black\")\n",
    "confusion_df=pd.DataFrame(LenderTest2(race_sex_less_data,black_data,'_black'), index=['true-','false-','false+','true+'])\n",
    "result_df=pd.concat([result_df, confusion_df],axis=1)\n",
    "# #results for the effect of race white\n",
    "print(\"white\")\n",
    "confusion_df=pd.DataFrame(LenderTest2(race_sex_less_data,white_data,'_white'), index=['true-','false-','false+','true+'])\n",
    "result_df=pd.concat([result_df, confusion_df],axis=1)\n",
    "\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df=result_df.reset_index()\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv('confpercentLogregress.csv', index=False,header=True,encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
